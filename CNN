{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qusetion 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a). Please fill in the missing code to train a 3-layer ConvNet on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.308548\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.331400\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.235070\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.210311\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.159349\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.258880\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.206248\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 2.210301\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.050405\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 2.194498\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.289950\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 2.015035\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.211916\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 2.141346\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.209134\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.888746\n",
      "\n",
      "Validation set: Average loss: 2.1753, Accuracy: 2812/10000 (28.12%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.144143\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 2.041140\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.156553\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 2.178673\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.230532\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 2.180688\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.050972\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 2.112114\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.133033\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 2.065740\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.184233\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 2.171285\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 2.155424\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 2.166653\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 2.056216\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 2.239439\n",
      "\n",
      "Validation set: Average loss: 2.1366, Accuracy: 3217/10000 (32.17%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_transform =transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=train_transform)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# for (x_train, y_train) in train_loader:\n",
    "#     print (x_train.size())\n",
    "#     break\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*32*32, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        ## flatten operation\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        ## forward x to softmax activation function.\n",
    "        output = F.softmax(x, dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def train(epoch,log_interval=100):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        ## zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        ## backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        ## update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        \n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "## define the SGD optimizer with lr=0.01 and momentum=0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 2\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b). Please apply knowledge distillation by using ResNet18 as a teacher network to improve the accuracy of the above 3-layer ConvNet, which can be treated as a student network. Please fill in the missing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [3168/50000 (6%)]\tLoss_kd: 0.854856\tLoss_class: 1.067331\tLoss: 1.922187\n",
      "Train Epoch: 1 [6368/50000 (13%)]\tLoss_kd: 0.701941\tLoss_class: 1.048110\tLoss: 1.750051\n",
      "Train Epoch: 1 [9568/50000 (19%)]\tLoss_kd: 0.785347\tLoss_class: 1.106295\tLoss: 1.891642\n",
      "Train Epoch: 1 [12768/50000 (26%)]\tLoss_kd: 0.734804\tLoss_class: 0.992062\tLoss: 1.726865\n",
      "Train Epoch: 1 [15968/50000 (32%)]\tLoss_kd: 0.771739\tLoss_class: 1.110948\tLoss: 1.882688\n",
      "Train Epoch: 1 [19168/50000 (38%)]\tLoss_kd: 0.699138\tLoss_class: 1.024544\tLoss: 1.723682\n",
      "Train Epoch: 1 [22368/50000 (45%)]\tLoss_kd: 0.698171\tLoss_class: 1.027328\tLoss: 1.725499\n",
      "Train Epoch: 1 [25568/50000 (51%)]\tLoss_kd: 0.773947\tLoss_class: 1.109962\tLoss: 1.883909\n",
      "Train Epoch: 1 [28768/50000 (58%)]\tLoss_kd: 0.766216\tLoss_class: 1.086311\tLoss: 1.852527\n",
      "Train Epoch: 1 [31968/50000 (64%)]\tLoss_kd: 0.764030\tLoss_class: 1.051860\tLoss: 1.815889\n",
      "Train Epoch: 1 [35168/50000 (70%)]\tLoss_kd: 0.740818\tLoss_class: 1.063382\tLoss: 1.804201\n",
      "Train Epoch: 1 [38368/50000 (77%)]\tLoss_kd: 0.823720\tLoss_class: 1.062478\tLoss: 1.886198\n",
      "Train Epoch: 1 [41568/50000 (83%)]\tLoss_kd: 0.776136\tLoss_class: 1.010355\tLoss: 1.786491\n",
      "Train Epoch: 1 [44768/50000 (90%)]\tLoss_kd: 0.744671\tLoss_class: 1.090781\tLoss: 1.835452\n",
      "Train Epoch: 1 [47968/50000 (96%)]\tLoss_kd: 0.704260\tLoss_class: 0.995946\tLoss: 1.700206\n",
      "\n",
      "Validation set: Average loss: 2.0911, Accuracy: 3654/10000 (36.54%)\n",
      "\n",
      "Train Epoch: 2 [3168/50000 (6%)]\tLoss_kd: 0.733948\tLoss_class: 1.068212\tLoss: 1.802160\n",
      "Train Epoch: 2 [6368/50000 (13%)]\tLoss_kd: 0.774394\tLoss_class: 1.015011\tLoss: 1.789405\n",
      "Train Epoch: 2 [9568/50000 (19%)]\tLoss_kd: 0.725155\tLoss_class: 1.019224\tLoss: 1.744379\n",
      "Train Epoch: 2 [12768/50000 (26%)]\tLoss_kd: 0.757028\tLoss_class: 1.050766\tLoss: 1.807793\n",
      "Train Epoch: 2 [15968/50000 (32%)]\tLoss_kd: 0.746992\tLoss_class: 1.018865\tLoss: 1.765857\n",
      "Train Epoch: 2 [19168/50000 (38%)]\tLoss_kd: 0.741571\tLoss_class: 0.980394\tLoss: 1.721965\n",
      "Train Epoch: 2 [22368/50000 (45%)]\tLoss_kd: 0.758298\tLoss_class: 0.982245\tLoss: 1.740543\n",
      "Train Epoch: 2 [25568/50000 (51%)]\tLoss_kd: 0.789322\tLoss_class: 0.964234\tLoss: 1.753556\n",
      "Train Epoch: 2 [28768/50000 (58%)]\tLoss_kd: 0.789928\tLoss_class: 1.002153\tLoss: 1.792082\n",
      "Train Epoch: 2 [31968/50000 (64%)]\tLoss_kd: 0.721768\tLoss_class: 1.025833\tLoss: 1.747601\n",
      "Train Epoch: 2 [35168/50000 (70%)]\tLoss_kd: 0.682484\tLoss_class: 0.980097\tLoss: 1.662582\n",
      "Train Epoch: 2 [38368/50000 (77%)]\tLoss_kd: 0.747546\tLoss_class: 1.063203\tLoss: 1.810749\n",
      "Train Epoch: 2 [41568/50000 (83%)]\tLoss_kd: 0.711991\tLoss_class: 1.027174\tLoss: 1.739164\n",
      "Train Epoch: 2 [44768/50000 (90%)]\tLoss_kd: 0.809741\tLoss_class: 0.989566\tLoss: 1.799307\n",
      "Train Epoch: 2 [47968/50000 (96%)]\tLoss_kd: 0.804218\tLoss_class: 1.038032\tLoss: 1.842250\n",
      "\n",
      "Validation set: Average loss: 2.0187, Accuracy: 4400/10000 (44.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import resnet\n",
    "\n",
    "\n",
    "## load the saved model for ResNet-18 so you do not to train ResNet-18\n",
    "def load_checkpoint(checkpoint, model):\n",
    "\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise(\"File doesn't exist {}\".format(checkpoint))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "def cal_loss_kd(T, alpha, output_student, output_teacher):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "    \n",
    "    Hint: since the built-in CrossEntropy loss in pytorch only takes “(output, target)” \n",
    "    where the target is the class index, which is different from knowledge distillation loss,\n",
    "    you can consider KL Divergence in pytorch. Please find out the definition of CrossEntropy,\n",
    "    KL Divergence, their difference and usage in pytorch to carefully complete this part.\n",
    "\n",
    "    \"\"\"\n",
    "    criterion_kd = nn.KLDivLoss()\n",
    "    \n",
    "    ## calculate the soft distribution of the student output with temperature T\n",
    "    student_soft = F.log_softmax(output_student / T, dim=1)\n",
    "    \n",
    "    ## calculate the soft distribution of the teacher output with temperature T\n",
    "    teacher_soft = F.softmax(output_teacher / T, dim=1)\n",
    "    \n",
    "    KD_loss = criterion_kd(student_soft,teacher_soft)* (alpha * T * T)\n",
    "    \n",
    "    return KD_loss\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_transform =transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=train_transform)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "teacher_model = resnet.ResNet18().to(device)\n",
    "teacher_checkpoint = 'best.pth.tar'\n",
    "load_checkpoint(teacher_checkpoint, teacher_model)\n",
    "teacher_model.eval()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*32*32, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        ## flatten operation\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        ## forward x to softmax activation function.\n",
    "        output = F.softmax(x, dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def train(epoch, log_interval=100):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        ## zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## forward data through the student model\n",
    "        output_student = model(data)\n",
    "\n",
    "        ## forward data through the teacher model\n",
    "        output_teacher = teacher_model(data)\n",
    "\n",
    "        ## set the output of the teacher attribute .requires_grad as False\n",
    "        output_teacher = Variable(output_teacher, requires_grad=False)\n",
    "\n",
    "        T=5\n",
    "        alpha = 0.5\n",
    "\n",
    "        loss_kd = cal_loss_kd(T, alpha, output_student, output_teacher)\n",
    "\n",
    "        loss_class = criterion(output_student, target)*(1-alpha)\n",
    "\n",
    "        loss = loss_kd + loss_class\n",
    "\n",
    "        ## backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        ## update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss_kd: {:.6f}\\tLoss_class: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss_kd.data.item(),loss_class.data.item(),loss.data.item()),flush=True)\n",
    "\n",
    "\n",
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "## define the SGD optimizer with lr=0.01 and momentum=0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss for the student model. Please note that we do not use softmax output in the network.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 2\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
