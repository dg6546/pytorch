{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qusetion 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a). Please fill in the missing code to train 3 different MLPs. And then compare their accuracy values by plotting a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([ transforms.ToTensor(),\n",
    "                                    ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    ])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=train_transform)\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=test_transform)\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=5,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "def train(epoch, log_interval=200):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        ## zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## pass data through the network\n",
    "        outputs = model(data)\n",
    "\n",
    "        ## calculate loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        ## backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        ## update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        \n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lion\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "<ipython-input-4-856e43e1bfb6>:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307893\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.300631\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.297479\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.302815\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.296555\n",
      "\n",
      "Validation set: Average loss: 2.2955, Accuracy: 1135/10000 (11.35%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.297109\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.298548\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.292517\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.294955\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.254480\n",
      "\n",
      "Validation set: Average loss: 2.2792, Accuracy: 1703/10000 (17.03%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.301501\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.277588\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.279097\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.256448\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.220828\n",
      "\n",
      "Validation set: Average loss: 2.2274, Accuracy: 2141/10000 (21.41%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.247127\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.236233\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.213390\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.157149\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.220932\n",
      "\n",
      "Validation set: Average loss: 2.1723, Accuracy: 2968/10000 (29.68%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.123981\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.161742\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.225153\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.169327\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.134794\n",
      "\n",
      "Validation set: Average loss: 2.1300, Accuracy: 3085/10000 (30.85%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-856e43e1bfb6>:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301659\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295807\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.287335\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.254032\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.204195\n",
      "\n",
      "Validation set: Average loss: 2.1574, Accuracy: 4591/10000 (45.91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.181053\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.975865\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.952410\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.875453\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.934577\n",
      "\n",
      "Validation set: Average loss: 1.8507, Accuracy: 6595/10000 (65.95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.871477\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.887918\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.820065\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.899580\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.874031\n",
      "\n",
      "Validation set: Average loss: 1.8085, Accuracy: 6697/10000 (66.97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.795615\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.788044\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.842791\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.768172\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.807477\n",
      "\n",
      "Validation set: Average loss: 1.7429, Accuracy: 7484/10000 (74.84%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.808311\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.782602\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.808048\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.702550\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.718758\n",
      "\n",
      "Validation set: Average loss: 1.7026, Accuracy: 7938/10000 (79.38%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-856e43e1bfb6>:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305587\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295967\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.278824\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.243491\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.153301\n",
      "\n",
      "Validation set: Average loss: 2.1332, Accuracy: 4235/10000 (42.35%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.121574\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.077728\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.021989\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.952687\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.921813\n",
      "\n",
      "Validation set: Average loss: 1.8758, Accuracy: 6385/10000 (63.85%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.892058\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.971460\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.838469\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.746615\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.757621\n",
      "\n",
      "Validation set: Average loss: 1.7730, Accuracy: 7453/10000 (74.53%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.821830\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.797745\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.715070\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.844924\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.732026\n",
      "\n",
      "Validation set: Average loss: 1.7121, Accuracy: 8009/10000 (80.09%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.730908\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.787347\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.682155\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.694475\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.686120\n",
      "\n",
      "Validation set: Average loss: 1.6864, Accuracy: 8116/10000 (81.16%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAFdCAYAAADWns55AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7xVdZ3/8deHi1cUdQSHi6hTKgol6gm0i5mElpdUSpPJibzx616OjFmP3++H9agZqt9EU/qrLC362WiaGtY4pECpM4qGeSzMzLsSKJiQKKBcPr8/1jq2hXPggOy9F2e/no/Hfpx135+1z4L3+X7X2mtFZiJJkqqhV7MLkCRJf2UwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGs9RCIuItEfFQRLwQEac06D0vjogrG/FejRIR+0ZERkSfZteinsdgVkuJiF9FxNKI2L7ZtTTJF4BLMrNfZv602cVsbWVYvr7ZdUivhcGslhER+wJvAxJ4T4Pfuyotq32A+5tdxLasQr9L9VAGs1rJB4G5wA+AibUzImLviLg+IpZExJ8j4pKaeedFxAMRsTwifh8Rh5XTX9U6i4gfRMQXy+GjI2JBRHwmIp4Gvh8Ru0fEz8v3WFoOD61Zf4+I+H5ELCzn/7ScPj8iTqpZrm9EPBsRozrbybLehyPiuYi4MSIGl9MfAf4O+FnZlb1Br0FEDI6I68oaH4uIT9bMGx0Rd0bEsohYFBGXRMR2NfNHRMQt5fs+ExGfq9n0dhHxw/IzvD8i2rr6JZWf64fLLvelEXFpRETN/LPL38fSiPhFROxTTr+tXOS+cv/eHxG3RsR7y/lvLbd9fDn+zohoL4d7RcT/jIgnImJxWWv/cl5Ht/U5EfEkMKeTmt8bEY9HxMiu9kvqLoNZreSDwI/K13ERsRdARPQGfg48AewLDAGuLuedBlxcrrsrRUv7z918v78F9qBopU6i+Pf2/XJ8GLASuKRm+f8H7ASMAAYC08rpPwTOrFnueGBRZrav/4YRcQzwL8DpwKByn64GyMzXAU8CJ5Vd2S+tt24v4GfAfeVnMBb4dEQcVy6yFjgf2BM4spz/0XLdXYBZwExgMPB6YHbN5t9T1rEbcON6+92ZE4E3AYeU+3Jc+T6nAJ8DxgMDgNuBq8r9O6pc95By/34M3AocXU4/CngUeHvN+K3l8IfK1zso/njp10mNbwcO6qilQ0ScBXwZeGdmzt/Efkmblpm+fPX4F/BWYDWwZzn+B+D8cvhIYAnQp5P1fgF8qottJvD6mvEfAF8sh48GXgZ22EhNo4Cl5fAgYB2weyfLDQaWA7uW4z8BLuxim5cDX6kZ71fu977l+OMUAdLZumOAJ9eb9lng+10s/2nghnJ4AnBvF8tdDMyqGT8YWLmRzyWBt9aMXwNcVA7/J3BOzbxewApgny5+J2OB35bDM4Fzgbnl+K3A+HJ4NvDRmvUOLD+3PhR/rCXwdzXzO6ZNBn4PDG32Me6r57xsMatVTARuzsxny/F/56/d2XsDT2Tmmk7W2xt4ZAvfc0lmruoYiYidIuI7ZXfp88BtwG5li31v4LnMXLr+RjJzIfDfwHsjYjfg3RSt/s4Mpmgld6z7AkULf0g36t0HGFx2VS+LiGUUrdOOnoUDyu73p8v6/5mi9Qyb/pyerhleAeywiXO16y/fr6bGf6up7zkgNrJ/dwIHlL0joyh6H/aOiD2B0RS/A1jvcyuH+1Due+mpTrb/T8ClmblgI/sibRYvYlCPFxE7UnSH9i7P9wJsTxGKh1D8hzssIvp0Es5PAa/rYtMrKLqeO/wtUPsf9PqPbruAoiU2JjOfLs8R30sRLE8Be0TEbpm5rJP3mk7R2usD3JmZf+qipoUU4QVAROwM/A3Q1fK1ngIey8z9u5j/rbLeCZm5PCI+DbyvZt0J3XiP1+op4EuZ2dUfJq+SmSsi4h7gU8D8zHw5Iu4A/hF4pOYPtVd9bhSnGtYAzwAd1wF09ii+Y4GZEfF0Zl63+bsjbcgWs1rBKRTnRw+maDWNojhXeDvFueO7gUXA1IjYOSJ2iIi3lOt+D5gcEYdH4fUdFxsB7cDfR0TviHgXfz132ZVdKM4rL4uIPYApHTMycxFFN+3/jeIisb4RcVTNuj8FDqMImB9u5D3+HTgrIkaVF3f9M3BXZj6+idqg+Byej+KCtR3L/RoZEW+qqf954IWIGA58pGbdnwN/GxGfjojtI2KXiBjTjffcXN8GPhsRIwAion95HUCHZyjOEde6Ffg4fz2f/Kv1xqE4T31+ROwXEf0oPrcfd9GLUut+4F3ApRHR0Cv91XMZzGoFEynOkz6ZmU93vCgu7vkARYv1JIoLlp6kaPW+HyAzrwW+RBF4yykCco9yu58q11tWbmdT3wv+OrAj8CzF1eEz15v/DxTnNf8ALKY4h0tZx0rgOmA/4Pqu3iAzZwP/q1x2EUVr/4xN1NWx7tpyf0YBj5V1fg/oXy4yGfh7is/hu8CPa9ZdDowr138aeIjiQqqtKjNvoLjQ6uqyO30+Rdd+h4uB6WVX9+nltFsp/qi4rYtxgCsoLr67jWLfVwGf6GZN91FcrPbdiHj3ppaXNiUyO+udkVQ1EfG/gQMy88xNLixpm+U5ZmkbUHZ9n0PRqpbUg9WtKzsirii/qD+/ZtoeUdyA4KHy5+418z4bxU0RHqz53qTU8iLiPIqLnv4zM2/b1PKStm1168ouL1x5AfhhZo4sp32F4ishUyPiIorvbH4mIg6muPhiNMXXFmZRdNmtrUtxkiRVVN1azOVf9s+tN/lkiq99UP48pWb61Zn5UmY+BjxMEdKSJLWURl+VvVf5tZCOr4cMLKcP4dVf3l9A926IIElSj1KVi7+ik2md9rFHxCSK+w6z8847Hz58+PB61iVJ0lZ3zz33PJuZAzqb1+hgfiYiBmXmoogYRPFdTShayHvXLDeU4k48G8jMy4DLANra2nLevHn1rFeSpK0uIp7oal6ju7Jv5K/3J54IzKiZfkZ5x6D9gP0p7kIkSVJLqVuLOSKuonjCzp4RsYDi9oNTgWsi4hyKOyydBpCZ90fENRRPaVkDfMwrsiVJrahuwZyZXd3QfmwXy3+J4taHkiS1LO+VLUlShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySpA1MmzaNESNGMHLkSCZMmMCqVau49tprGTFiBL169WLevHldrnv22WczcOBARo4cucG8b37zmxx44IGMGDGCCy+8sJ67sM0ymCVJr/KnP/2Jb3zjG8ybN4/58+ezdu1arr76akaOHMn111/PUUcdtdH1P/ShDzFz5swNpv/yl79kxowZ/Pa3v+X+++9n8uTJ9dqFbVqfZhcgSaqeNWvWsHLlSvr27cuKFSsYPHgwBx10ULfWPeqoo3j88cc3mP6tb32Liy66iO233x6AgQMHbs2SewxbzJKkVxkyZAiTJ09m2LBhDBo0iP79+3Pssce+5u3+8Y9/5Pbbb2fMmDG8/e1v59e//vVWqLbnMZglSa+ydOlSZsyYwWOPPcbChQt58cUXufLKK1/zdtesWcPSpUuZO3cuX/3qVzn99NPJzK1Qcc9iMEst4MEHH2TUqFGvvHbddVe+/vWvc99993HkkUfyhje8gZNOOonnn39+g3VXrVrF6NGjOeSQQxgxYgRTpkx5ZV57eztHHHEEo0aNoq2tjbvvvruRu6U6mTVrFvvttx8DBgygb9++jB8/njvuuOM1b3fo0KGMHz+eiGD06NH06tWLZ599ditU3LMYzFILOPDAA2lvb6e9vZ177rmHnXbaiVNPPZVzzz2XqVOn8rvf/Y5TTz2Vr371qxusu/322zNnzhzuu+8+2tvbmTlzJnPnzgXgwgsvZMqUKbS3t/OFL3zBq2x7iGHDhjF37lxWrFhBZjJ79uxun1/emFNOOYU5c+YARbf2yy+/zJ577vmat9vTGMxSi5k9ezave93r2GeffXjwwQdfucJ23LhxXHfddRssHxH069cPgNWrV7N69Woi4pV5Ha3sv/zlLwwePLhBe6F6GjNmDO973/s47LDDeMMb3sC6deuYNGkSN9xwA0OHDuXOO+/khBNO4LjjjgNg4cKFHH/88a+sP2HCBI488kgefPBBhg4dyuWXXw4UX6N69NFHGTlyJGeccQbTp09/5VjSX8W23L/f1taWG/sunaQNnX322Rx22GF8/OMf581vfjOf+cxnOPnkk/na177GlClTWL58+QbrrF27lsMPP5yHH36Yj33sY3z5y18G4IEHHuC4444jM1m3bh133HEH++yzT6N3SdrmRMQ9mdnW2TxbzFILefnll7nxxhs57bTTALjiiiu49NJLOfzww1m+fDnbbbddp+v17t2b9vZ2FixYwN133838+fOB4usv06ZN46mnnmLatGmcc845DdsXqadqSos5Is4HzgUS+B1wFrAT8GNgX+Bx4PTMXLqx7dhiljbPjBkzuPTSS7n55ps3mPfHP/6RM888c5MXcH3+859n5513ZvLkyfTv359ly5YREWQm/fv37/QCMsG+F/1Hs0vQa/T41BO22rYq1WKOiCHAJ4G2zBwJ9AbOAC4CZmfm/sDsclzSVnTVVVcxYcKEV8YXL14MwLp16/jiF7/Ihz/84Q3WWbJkCcuWLQNg5cqVzJo1i+HDhwMwePBgbr31VgDmzJnD/vvvX+9dkHq8ZnVl9wF2jIg+FC3lhcDJwPRy/nTglCbVJvVIK1as4JZbbmH8+PGvTLvqqqs44IADGD58OIMHD+ass84CXn0xz6JFi3jHO97BG9/4Rt70pjcxbtw4TjzxRAC++93vcsEFF3DIIYfwuc99jssuu6zxOyb1MM3qyv4U8CVgJXBzZn4gIpZl5m41yyzNzN07WXcSMAlg2LBhhz/xxBONKluStphd2du+ntyVvTtF63g/YDCwc0Sc2d31M/OyzGzLzLYBAwbUq0xJkpqiGQ+xeCfwWGYuAYiI64E3A89ExKDMXBQRg4DFTahN6jZbQNu2rdn6kbamZpxjfhI4IiJ2iuKb5WOBB4AbgYnlMhOBGU2oTZKkpmp4izkz74qInwC/AdYA9wKXAf2AayLiHIrwPq3RtUmS1GxNeR5zZk4Bpqw3+SWK1rMkSS3LO39JklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkV0pRgjojdIuInEfGHiHggIo6MiD0i4paIeKj8uXszapMkqZma1WL+N2BmZg4HDgEeAC4CZmfm/sDsclySpJbS8GCOiF2Bo4DLATLz5cxcBpwMTC8Xmw6c0ujaJElqtma0mP8OWAJ8PyLujYjvRcTOwF6ZuQig/DmwCbVJktRUzQjmPsBhwLcy81DgRTaj2zoiJkXEvIiYt2TJknrVKElSUzQjmBcACzLzrnL8JxRB/UxEDAIofy7ubOXMvCwz2zKzbcCAAQ0pWJKkRml4MGfm08BTEXFgOWks8HvgRmBiOW0iMKPRtUmS1Gx9mvS+nwB+FBHbAY8CZ1H8kXBNRJwDPAmc1qTaJElqmqYEc2a2A22dzBrb6FokSaoS7/wlSVKFGMySJFWIwSxJUoVsMpgj4sSIMMAlSWqA7gTuGcBDEfGViDio3gVJktTKNhnMmXkmcCjwCMVtNO8s7761S92rkySpxXSrizoznweuA64GBgGnAr+JiE/UsTZJklpOd84xnxQRNwBzgL7A6Mx8N8XjGifXuT5JklpKd24wchowLTNvq52YmSsi4uz6lCVJUmvqTjBPARZ1jETEjhSPaHw8M2fXrTJJklpQd84xXwusqxlfW06TJElbWXeCuU9mvtwxUg5vV7+SJElqXd0J5iUR8Z6OkYg4GXi2fiVJktS6unOO+cMUj2i8BAjgKeCDda1KkqQWtclgzsxHgCMioh8Qmbm8/mVJktSauvU85og4ARgB7BARAGTmF+pYlyRJLak7Nxj5NvB+4BMUXdmnAfvUuS5JklpSdy7+enNmfhBYmpmfB44E9q5vWZIktabuBPOq8ueKiBgMrAb2q19JkiS1ru6cY/5ZROwGfBX4DZDAd+talSRJLWqjwRwRvYDZmbkMuC4ifg7skJl/aUh1kiS1mI12ZWfmOuBfa8ZfMpQlSaqf7pxjvjki3hsd35OSJEl1051zzP8I7AysiYhVFF+Zyszcta6VSZLUgrpz569dGlGIJEnqRjBHxFGdTc/M27Z+OZIktbbudGX/U83wDsBo4B7gmLpUJElSC+tOV/ZJteMRsTfwlbpVJElSC+vOVdnrWwCM3NqFSJKk7p1j/ibF3b6gCPJRwH31LEqSpFbVnXPM82qG1wBXZeZ/16keSZJaWneC+SfAqsxcCxARvSNip8xcUd/SJElqPd05xzwb2LFmfEdgVn3KkSSptXUnmHfIzBc6RsrhnepXkiRJras7wfxiRBzWMRIRhwMr61eSJEmtqzvnmD8NXBsRC8vxQcD761eSJEmtqzs3GPl1RAwHDqR4gMUfMnN13SuTJKkFbbIrOyI+BuycmfMz83dAv4j4aP1LkySp9XTnHPN5mbmsYyQzlwLn1a8kSZJaV3eCuVdERMdIRPQGtqtfSZIkta7uXPz1C+CaiPg2xa05Pwz8Z12rkiSpRXUnmD8DTAI+QnHx170UV2ZLkqStbJNd2Zm5DpgLPAq0AWOBB+pclyRJLanLFnNEHACcAUwA/gz8GCAz39GY0iRJaj0b68r+A3A7cFJmPgwQEec3pCpJklrUxrqy3ws8DfwyIr4bEWMpzjFLkqQ66TKYM/OGzHw/MBz4FXA+sFdEfCsijm1QfZIktZTuXPz1Ymb+KDNPBIYC7cBFda9MkqQW1J0bjLwiM5/LzO9k5jH1KkiSpFa2WcG8NUVE74i4NyJ+Xo7vERG3RMRD5c/dm1WbJEnN0rRgBj7Fq78PfREwOzP3B2Zjd7kkqQU1JZgjYihwAvC9msknA9PL4enAKY2uS5KkZmtWi/nrwIXAupppe2XmIoDy58BmFCZJUjM1PJgj4kRgcWbes4XrT4qIeRExb8mSJVu5OkmSmqsZLea3AO+JiMeBq4FjIuJK4JmIGARQ/lzc2cqZeVlmtmVm24ABAxpVsyRJDdHwYM7Mz2bm0Mzcl+Je3HMy80zgRmBiudhEYEaja5MkqdmaeVX2+qYC4yLiIWBcOS5JUkvpzvOY6yYzf0Vxu08y888Uj5SUJKllVanFLElSyzOYJUmqEINZkqQKMZglSaoQg1mSpAoxmCVJqhCDWZKkCjGYJUmqEINZkqQKMZglSaoQg3kbtWrVKkaPHs0hhxzCiBEjmDJlCgDPPfcc48aNY//992fcuHEsXbq00/WnTZvGiBEjGDlyJBMmTGDVqlUAXHzxxQwZMoRRo0YxatQobrrppobtkyTJYN5mbb/99syZM4f77ruP9vZ2Zs6cydy5c5k6dSpjx47loYceYuzYsUyduuGzQP70pz/xjW98g3nz5jF//nzWrl3L1Vdf/cr8888/n/b2dtrb2zn++OMbuVuS1PIM5m1URNCvXz8AVq9ezerVq4kIZsyYwcSJxdMzJ06cyE9/+tNO11+zZg0rV65kzZo1rFixgsGDBzesdklS1wzmbdjatWsZNWoUAwcOZNy4cYwZM4ZnnnmGQYMGATBo0CAWL168wXpDhgxh8uTJDBs2jEGDBtG/f3+OPfbYV+ZfcsklvPGNb+Tss8/usitcklQfBvM2rHfv3rS3t7NgwQLuvvtu5s+f3631li5dyowZM3jsscdYuHAhL774IldeeSUAH/nIR3jkkUdob29n0KBBXHDBBfXcBUnSegzmHmC33Xbj6KOPZubMmey1114sWrQIgEWLFjFw4MANlp81axb77bcfAwYMoG/fvowfP5477rgDgL322ovevXvTq1cvzjvvPO6+++6G7osktTqDeRu1ZMkSli1bBsDKlSuZNWsWw4cP5z3veQ/Tp08HYPr06Zx88skbrDts2DDmzp3LihUryExmz57NQQcdBPBKqAPccMMNjBw5sgF7I0nq0KfZBWjLLFq0iIkTJ7J27VrWrVvH6aefzoknnsiRRx7J6aefzuWXX86wYcO49tprAVi4cCHnnnsuN910E2PGjOF973sfhx12GH369OHQQw9l0qRJAFx44YW0t7cTEey777585zvfaeZuSlLLicxsdg1brK2tLefNm9fsMtSi9r3oP5pdgl6Dx6ee0ND383jZ9m3NYyYi7snMts7m2ZUtSVKFGMySJFWI55hr2NW0bWt016Qk1YMtZkmSKsRgliSpQgxmSZIqxGCWJKlCDGZJkirEYJYkqUIMZkmSKsRgliSpQgxmSZIqxGCWJKlCDGZJkirEYJYkqUIMZkmSKsRgliSpQgxmSZIqxGCWJKlCDGZJkirEYJYkqUIMZkmSKsRgliSpQgxmSZIqxGCWJKlCDGZJkirEYJYkqUIMZkmSKsRgliSpQgxmSZIqpOHBHBF7R8QvI+KBiLg/Ij5VTt8jIm6JiIfKn7s3ujZJkpqtGS3mNcAFmXkQcATwsYg4GLgImJ2Z+wOzy3FJklpKw4M5Mxdl5m/K4eXAA8AQ4GRgernYdOCURtcmSVKzNfUcc0TsCxwK3AXslZmLoAhvYGAX60yKiHkRMW/JkiWNKlWSpIZoWjBHRD/gOuDTmfl8d9fLzMsysy0z2wYMGFC/AiVJaoKmBHNE9KUI5R9l5vXl5GciYlA5fxCwuBm1SZLUTM24KjuAy4EHMvNrNbNuBCaWwxOBGY2uTZKkZuvThPd8C/APwO8ior2c9jlgKnBNRJwDPAmc1oTaJElqqoYHc2b+FxBdzB7byFokSaoa7/wlSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRViMEsSVKFGMySJFWIwSxJUoUYzJIkVYjBLElShRjMkiRVSOWCOSLeFREPRsTDEXFRs+uRJKmRKhXMEdEbuBR4N3AwMCEiDm5uVZIkNU6lghkYDTycmY9m5svA1cDJTa5JkqSGqVowDwGeqhlfUE6TJKkl9Gl2AeuJTqblqxaImARMKkdfiIgH615Vz7En8Gyzi6iX+HKzK+hxPF60uTxmum+frmZULZgXAHvXjA8FFtYukJmXAZc1sqieIiLmZWZbs+vQtsHjRZvLY2brqFpX9q+B/SNiv4jYDjgDuLHJNUmS1DCVajFn5pqI+DjwC6A3cEVm3t/ksiRJaphKBTNAZt4E3NTsOnooTwFoc3i8aHN5zGwFkZmbXkqSJDVE1c4xS5LU0gzmHiAiMiL+tWZ8ckRcXA5fHBErImJgzfwXaoaviIjFETG/oUWrqbb0mImIvSPilxHxQETcHxGfanjxarjXcLzsEBF3R8R95fHy+YYXvw0ymHuGl4DxEbFnF/OfBS7oYt4PgHfVoyhV2pYeM2uACzLzIOAI4GPeNrclbOnx8hJwTGYeAowC3hURR9Spxh7DYO4Z1lBcdHF+F/OvAN4fEXusPyMzbwOeq2NtqqYtOmYyc1Fm/qYcXg48gHfnawVberxkZnb00PUtX17YtAkGc89xKfCBiOjfybwXKP7h2O2oWq/pmImIfYFDgbvqUZwqZ4uOl4joHRHtwGLglsz0eNkEg7mHyMzngR8Cn+xikW8AEyNi18ZVpSp7LcdMRPQDrgM+XW5HPdyWHi+ZuTYzR1HcyXF0RIysb6XbPoO5Z/k6cA6w8/ozMnMZ8O/ARxtdlCpts4+ZiOhLEco/yszrG1GkKmOL/48p5/8Kr2nZJIO5B8nM54BrKP7hdOZrwP+ggjeWUXNs7jETEQFcDjyQmV9rSJGqjC04XgZExG7l8I7AO4E/NKDUbZrB3PP8K8UTXjaQmc8CNwDbd0yLiKuAO4EDI2JBRHT1D0491+YcM28B/gE4JiLay9fxjSlTFbE5x8sg4JcR8VuKZyHckpk/b0iV2zDv/CVJUoXYYpYkqUIMZkmSKsRgliSpQgxmSZIqxGCWJKlCDGapB9nYU4A2ss7REfHmOtTyoYi4ZGtvV+rpDGapZ9nUU4A6czSwVYM5IryJjbSFDGapZ+nyKUDlXZiui4hfl6+3lA+i+DBwfnmzkLdHxKNR2C0i1kXEUeX6t0fE6yNij4j4aUT8NiLmRsQby/kXR8RlEXEzxT2Va9/7hIi4czP/YJBakn/VSj3PpcBvI+Ir603/N2BaZv5XRAwDfpGZB0XEt4EXMvP/AETEH4GDgf2Ae4C3RcRdwNDMfDgivgncm5mnRMQxFCE8qnyPw4G3ZubKiPhQub1TgX8Ejs/MpfXccaknMJilHkpyj58AAAE9SURBVCYzn4+IjqcArayZ9U7g4OJ21wDsGhG7dLKJ24GjKIL5X4DzgFspbqkI8FbgveV7zYmIv6l5FOCNmVn7nu8A2oBjfQqV1D12ZUs9U2dPAeoFHJmZo8rXkMxc3sm6twNvA0YDNwG7UZyHvq2cH52s03Fv3xfXm/4osAtwwJbshNSKDGapB+riKUA3Ax/vGImIju7n5RTh2eEuiovB1mXmKqCd4olBt5fzbwM+UG7jaODZjbSGnwDGAz+MiBGvYZeklmEwSz3X+k8B+iTQVl609XuKi74AfgacWl789bbMfAl4Cphbzr+dIrh/V45f3LEdYCowcWNFZOaDFEF+bUS87rXvltSz+XQpSZIqxBazJEkVYjBLklQhBrMkSRViMEuSVCEGsyRJFWIwS5JUIQazJEkVYjBLklQh/x8Gs6J+Z/QOaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        ## forward x to the first fully connected layer.\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        ## forward x to sigmoid activation function.\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        ## forward x to the second fully connected layer.\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        ## forward x to softmax activation function.\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        ## forward x to the first fully connected layer.\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        ## forward x to the second fully connected layer.\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        ## forward x to softmax activation function.\n",
    "        x = F.softmax(x)\n",
    "        \n",
    "        return x    \n",
    "\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        ## forward x to the first fully connected layer.\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        ## forward x to the second fully connected layer.\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        ## forward x to softmax activation function.\n",
    "        x = F.softmax(x)\n",
    "        \n",
    "        return x   \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "acc_result = []\n",
    "model_label = [\"NN1\", \"NN2\", \"NN3\"]\n",
    "\n",
    "## Trainig and evaluating 1) MLP\n",
    "model = Net1().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result.append(accv[4].item())\n",
    "    \n",
    "## Trainig and evaluating 2) MLP\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result.append(accv[4].item())\n",
    "\n",
    "## Trainig and evaluating 3) MLP\n",
    "model = Net3().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result.append(accv[4].item())\n",
    "\n",
    "## Show barchart\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "bars = ax.bar(model_label, acc_result)\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x()+ 0.3, yval + 1, round(yval,2))\n",
    "ax.set_ylim(0, 100)\n",
    "plt.title('Accuracy of each network')\n",
    "plt.xlabel('Network')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b). Please change the batch size and learning rate to train (2) MLP (Net2）as above. And then compare their accuracy values by plotting a matrix with values and colormap. Please note that each time you change the setting and train the MLP, you need to initialize the model again (e.g. \"model = Net2().to(device)\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-c69cbaecee49>:136: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.330856\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.920848\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.829149\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.789532\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.758618\n",
      "\n",
      "Validation set: Average loss: 1.7019, Accuracy: 9259/10000 (92.59%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.723040\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.747517\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.723246\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.676026\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.674583\n",
      "\n",
      "Validation set: Average loss: 1.6252, Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.667649\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.626793\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.651531\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.613439\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.624928\n",
      "\n",
      "Validation set: Average loss: 1.5897, Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.604702\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.635617\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.634097\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.673001\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.611547\n",
      "\n",
      "Validation set: Average loss: 1.5679, Accuracy: 9552/10000 (95.52%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.571811\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.575802\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.580700\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.568562\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.554553\n",
      "\n",
      "Validation set: Average loss: 1.5523, Accuracy: 9585/10000 (95.85%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.287807\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.855362\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.783545\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.666229\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.675221\n",
      "\n",
      "Validation set: Average loss: 1.6314, Accuracy: 9353/10000 (93.53%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.663536\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.645725\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.678135\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.597419\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.602279\n",
      "\n",
      "Validation set: Average loss: 1.5769, Accuracy: 9533/10000 (95.33%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.626438\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.610262\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.593328\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.561961\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.589038\n",
      "\n",
      "Validation set: Average loss: 1.5446, Accuracy: 9610/10000 (96.10%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.626667\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.577580\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.583569\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.582200\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.533942\n",
      "\n",
      "Validation set: Average loss: 1.5310, Accuracy: 9668/10000 (96.68%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.541306\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.566655\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.554156\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.546433\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.565536\n",
      "\n",
      "Validation set: Average loss: 1.5246, Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295195\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.802737\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.715253\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.666721\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.602387\n",
      "\n",
      "Validation set: Average loss: 1.5672, Accuracy: 9549/10000 (95.49%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.660796\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.575867\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.527950\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.584740\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.570826\n",
      "\n",
      "Validation set: Average loss: 1.5296, Accuracy: 9673/10000 (96.73%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.567196\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.541641\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.549631\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.534808\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.527383\n",
      "\n",
      "Validation set: Average loss: 1.5170, Accuracy: 9695/10000 (96.95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.547585\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.583416\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.528372\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.537288\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.526767\n",
      "\n",
      "Validation set: Average loss: 1.5097, Accuracy: 9732/10000 (97.32%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.523872\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.514351\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.529870\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.539703\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.523240\n",
      "\n",
      "Validation set: Average loss: 1.5020, Accuracy: 9744/10000 (97.44%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303339\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.890538\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.821471\n",
      "\n",
      "Validation set: Average loss: 1.7821, Accuracy: 9009/10000 (90.09%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.802135\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.776734\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.755175\n",
      "\n",
      "Validation set: Average loss: 1.6987, Accuracy: 9238/10000 (92.38%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.730357\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.729989\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.696217\n",
      "\n",
      "Validation set: Average loss: 1.6535, Accuracy: 9358/10000 (93.58%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.692133\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.698018\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.665236\n",
      "\n",
      "Validation set: Average loss: 1.6247, Accuracy: 9438/10000 (94.38%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.681228\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.621930\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.621612\n",
      "\n",
      "Validation set: Average loss: 1.6006, Accuracy: 9496/10000 (94.96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311279\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.820179\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.750863\n",
      "\n",
      "Validation set: Average loss: 1.6958, Accuracy: 9242/10000 (92.42%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.729915\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.656740\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.652268\n",
      "\n",
      "Validation set: Average loss: 1.6197, Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.654518\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.600763\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.638224\n",
      "\n",
      "Validation set: Average loss: 1.5856, Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.606588\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.603987\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.597640\n",
      "\n",
      "Validation set: Average loss: 1.5620, Accuracy: 9558/10000 (95.58%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.599648\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.583446\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.592816\n",
      "\n",
      "Validation set: Average loss: 1.5502, Accuracy: 9598/10000 (95.98%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315541\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.740608\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.681289\n",
      "\n",
      "Validation set: Average loss: 1.6257, Accuracy: 9417/10000 (94.17%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.701597\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.629085\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.615623\n",
      "\n",
      "Validation set: Average loss: 1.5673, Accuracy: 9567/10000 (95.67%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.592401\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.592917\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.588152\n",
      "\n",
      "Validation set: Average loss: 1.5413, Accuracy: 9632/10000 (96.32%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.547555\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.557300\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.557675\n",
      "\n",
      "Validation set: Average loss: 1.5288, Accuracy: 9679/10000 (96.79%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.535299\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.533800\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.539626\n",
      "\n",
      "Validation set: Average loss: 1.5203, Accuracy: 9690/10000 (96.90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_result2 = []\n",
    "\n",
    "## Trainig and evaluating 2) MLP with the firt setting\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=5,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result2.append(accv[epochs-1].item())\n",
    "\n",
    "\n",
    "## Trainig and evaluating 2) MLP with the second setting\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=5,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result2.append(accv[epochs-1].item())\n",
    "\n",
    "## Trainig and evaluating 2) MLP with the third setting\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=5,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result2.append(accv[epochs-1].item())\n",
    "\n",
    "## Trainig and evaluating 2) MLP with the fourth setting\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=5,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result2.append(accv[epochs-1].item())\n",
    "\n",
    "## Trainig and evaluating 2) MLP with the fifth setting\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=1,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=1,pin_memory=True)\n",
    "\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result2.append(accv[epochs-1].item())\n",
    "\n",
    "## Trainig and evaluating 2) MLP with the sixth setting\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=5,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result2.append(accv[epochs-1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_acc = np.array(acc_result2)\n",
    "#reshape np array\n",
    "np_acc = np_acc.reshape(2,3)\n",
    "\n",
    "#Set labels\n",
    "column_labels = [0, 64, 128]\n",
    "row_labels = [0, 0.005, 0.01, 0.02]\n",
    "\n",
    "#Make axes and fig\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#Display actual values of row and column\n",
    "ax.set_xticklabels(row_labels, minor=False)\n",
    "ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "\n",
    "#set label and title\n",
    "plt.xlabel(\"learning rate\", labelpad = 10)\n",
    "plt.ylabel(\"batch size\")\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.title('Accuracy of each setting', pad = 40)\n",
    "\n",
    "#plot\n",
    "ax.matshow(np_acc, cmap=plt.cm.Blues)\n",
    "\n",
    "#add text\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        c = round(np_acc[j,i], 2)\n",
    "        ax.text(i, j, str(c), va='center', ha='center', color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use “dropout”, “batch normalization” and any “data augmentation” to train to improve the accuracy of (2) MLP (Net2）in Question 1 a. (Batch size is 64 and learning rate is 0.01). Please describe clearly your design choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-c69cbaecee49>:136: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311200\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.765095\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.687077\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.644986\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.618163\n",
      "\n",
      "Validation set: Average loss: 1.5674, Accuracy: 9578/10000 (95.78%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.578018\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.627326\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.572060\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.546303\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.555212\n",
      "\n",
      "Validation set: Average loss: 1.5273, Accuracy: 9670/10000 (96.70%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.528129\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.583488\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.563961\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.518882\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.517314\n",
      "\n",
      "Validation set: Average loss: 1.5133, Accuracy: 9705/10000 (97.05%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.512720\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.552867\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.524755\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.519256\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.519503\n",
      "\n",
      "Validation set: Average loss: 1.5009, Accuracy: 9745/10000 (97.45%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.507112\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.525964\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.504715\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.508589\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.517662\n",
      "\n",
      "Validation set: Average loss: 1.4990, Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.501201\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.552609\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.507736\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.498636\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.500825\n",
      "\n",
      "Validation set: Average loss: 1.4941, Accuracy: 9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.498290\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.517712\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.493421\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.482978\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.499496\n",
      "\n",
      "Validation set: Average loss: 1.4917, Accuracy: 9786/10000 (97.86%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.496914\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.520478\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.491198\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.493415\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.509198\n",
      "\n",
      "Validation set: Average loss: 1.4907, Accuracy: 9778/10000 (97.78%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.492615\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.519897\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.489995\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.482314\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.503489\n",
      "\n",
      "Validation set: Average loss: 1.4899, Accuracy: 9775/10000 (97.75%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.502421\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.501596\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.487272\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.485244\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.499796\n",
      "\n",
      "Validation set: Average loss: 1.4878, Accuracy: 9782/10000 (97.82%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.493201\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 1.481787\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.482947\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 1.479227\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.498063\n",
      "\n",
      "Validation set: Average loss: 1.4881, Accuracy: 9783/10000 (97.83%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.497998\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 1.515134\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.486170\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 1.481497\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.500541\n",
      "\n",
      "Validation set: Average loss: 1.4874, Accuracy: 9795/10000 (97.95%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.486650\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 1.507140\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.483024\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 1.484948\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 1.490919\n",
      "\n",
      "Validation set: Average loss: 1.4855, Accuracy: 9791/10000 (97.91%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.485238\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 1.514831\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.492509\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 1.479015\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.497140\n",
      "\n",
      "Validation set: Average loss: 1.4854, Accuracy: 9794/10000 (97.94%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.489836\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 1.480416\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.481071\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 1.479705\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.494240\n",
      "\n",
      "Validation set: Average loss: 1.4875, Accuracy: 9774/10000 (97.74%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.490727\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 1.493246\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.490446\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 1.478267\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.516421\n",
      "\n",
      "Validation set: Average loss: 1.4850, Accuracy: 9799/10000 (97.99%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.481685\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 1.484357\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 1.501047\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 1.478430\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 1.503691\n",
      "\n",
      "Validation set: Average loss: 1.4860, Accuracy: 9793/10000 (97.93%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.490398\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 1.494270\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.496819\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 1.477523\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.492040\n",
      "\n",
      "Validation set: Average loss: 1.4845, Accuracy: 9797/10000 (97.97%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.475183\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 1.480328\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.483165\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 1.477583\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 1.486960\n",
      "\n",
      "Validation set: Average loss: 1.4841, Accuracy: 9802/10000 (98.02%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.480177\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 1.471035\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 1.486669\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 1.479155\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 1.488631\n",
      "\n",
      "Validation set: Average loss: 1.4848, Accuracy: 9791/10000 (97.91%)\n",
      "\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomRotation(10),\n",
    "                                     #torchvision.transforms.RandomCrop(28, padding = 1),\n",
    "                                     transforms.ToTensor(),])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    ])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=train_transform)\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=test_transform)\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=5,pin_memory=True)\n",
    "\n",
    "def train(epoch, log_interval=200):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        ## zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## pass data through the network\n",
    "        outputs = model(data)\n",
    "\n",
    "        ## calculate loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        ## backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        ## update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validate(loss_vector, accuracy_vector):\n",
    "    pred_all = []\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        pred_all.extend(pred)\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))\n",
    "    pred_all = np.array(pred_all)\n",
    "    return pred_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.drop2 = nn.Dropout(0.1)\n",
    "        self.norm1 = nn.BatchNorm1d(128, momentum=0.1, eps=1e-05) #just default value...\n",
    "        self.norm2 = nn.BatchNorm1d(10, momentum=0.1, eps=1e-05) #just default value...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        ## forward x to the first fully connected layer.\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "\n",
    "        \n",
    "        ## forward x to relu activation function.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        #x = self.drop1(x)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        ## forward x to the second fully connected layer.\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        #x = self.drop2(x)\n",
    "        \n",
    "        ## forward x to softmax activation function.\n",
    "        x = F.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "## Trainig and evaluating 2) MLP\n",
    "model = Net2().to(device, non_blocking=True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    pred_all = validate(lossv, accv)\n",
    "np.save('results.npy',pred_all)\n",
    "\n",
    "print(np.shape(pred_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the missing code to use RBM to initialize the parameters of the (1) MLP (Net1) in Question 1 a. Compare the accuracy values with and without RBM pretraining by plotting a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RBM...\n",
      "Epoch Error (epoch=0): 0.0438\n",
      "Epoch Error (epoch=1): 0.0296\n",
      "Epoch Error (epoch=2): 0.0262\n",
      "Epoch Error (epoch=3): 0.0243\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-652fb285bd90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mepoch_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mnum_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVISIBLE_UNITS\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# flatten input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([ transforms.ToTensor(),\n",
    "                                    ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    ])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=train_transform)\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=test_transform)\n",
    "\n",
    "## construct the loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=1,pin_memory=True)\n",
    "\n",
    "## construct the loader for validation\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False,num_workers=1,pin_memory=True)\n",
    "\n",
    "\n",
    "class RBM():\n",
    "\n",
    "    def __init__(self, num_visible, num_hidden, k, learning_rate=0.1):\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights = torch.randn(num_visible, num_hidden) * 0.01\n",
    "        self.visible_bias = torch.ones(num_visible) * 0.01\n",
    "        self.hidden_bias = torch.zeros(num_hidden)\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def _random_probabilities(self, num):\n",
    "        random_probabilities = torch.rand(num)\n",
    "        return random_probabilities\n",
    "    \n",
    "## compute the probability of hidden nodes given visible nodes.\n",
    "    def compute_hidden(self, visible_probabilities):\n",
    "        \n",
    "        hidden_activations = torch.matmul(visible_probabilities, self.weights) + self.hidden_bias\n",
    "        \n",
    "        hidden_probabilities = self._sigmoid(hidden_activations)\n",
    "        return hidden_probabilities\n",
    "\n",
    "## compute the probability of visible nodes given hidden nodes.\n",
    "    def compute_visible(self, hidden_probabilities):\n",
    "        \n",
    "        visible_activations = torch.matmul(hidden_probabilities, self.weights.t()) + self.visible_bias\n",
    "        \n",
    "        visible_probabilities = self._sigmoid(visible_activations)\n",
    "        return visible_probabilities\n",
    "    \n",
    "## Contrastive Divergence (CD-k)\n",
    "    def contrastive_divergence(self, input_data):\n",
    "        \n",
    "        positive_hidden_probabilities = self.compute_hidden(input_data)\n",
    "        \n",
    "        ## sample a hidden activation vector from its probability distribution\n",
    "        positive_hidden_activations = (positive_hidden_probabilities >= self._random_probabilities(self.num_hidden)).float()\n",
    "        \n",
    "        ## compute the positive gradient\n",
    "        positive_associations = torch.matmul(input_data.t(), positive_hidden_activations)\n",
    "\n",
    "        hidden_activations = positive_hidden_activations\n",
    "\n",
    "        for step in range(self.k):\n",
    "            visible_probabilities = self.compute_visible(hidden_activations)\n",
    "            hidden_probabilities = self.compute_hidden(visible_probabilities)\n",
    "            \n",
    "            ## resample a hidden activation vector from its probability distribution\n",
    "            hidden_activations = (hidden_probabilities >= self._random_probabilities(self.num_hidden)).float()\n",
    "\n",
    "        negative_visible_probabilities = visible_probabilities\n",
    "        negative_hidden_probabilities = hidden_probabilities\n",
    "        \n",
    "        ## compute the nagetive gradient\n",
    "        negative_associations = torch.matmul(negative_visible_probabilities.t(), negative_hidden_probabilities)\n",
    "        \n",
    "        batch_size = input_data.size(0)\n",
    "        \n",
    "        ## update weights\n",
    "        self.weights += (positive_associations - negative_associations) * (self.learning_rate / batch_size)\n",
    "        \n",
    "        ## update bias of visible units\n",
    "        self.visible_bias += torch.sum(input_data - negative_visible_probabilities, dim=0) * (self.learning_rate / batch_size)\n",
    "\n",
    "        ## update bias of hidden units\n",
    "        self.hidden_bias += torch.sum(positive_hidden_probabilities - negative_hidden_probabilities, dim=0) * (self.learning_rate / batch_size)\n",
    "        \n",
    "        ## compute reconstruction error\n",
    "        error = torch.mean((input_data - negative_visible_probabilities)**2)\n",
    "\n",
    "        return error\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VISIBLE_UNITS = 784  # 28 x 28 images\n",
    "HIDDEN_UNITS = 128\n",
    "CD_K = 3\n",
    "EPOCHS = 5\n",
    "\n",
    "########## TRAINING RBM ##########\n",
    "print('Training RBM...')\n",
    "\n",
    "rbm = RBM(VISIBLE_UNITS, HIDDEN_UNITS, CD_K)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_error = 0.0\n",
    "    num_batch = 0\n",
    "    for batch, _ in train_loader:\n",
    "        batch = batch.view(len(batch), VISIBLE_UNITS)  # flatten input data\n",
    "\n",
    "        batch_error = rbm.contrastive_divergence(batch)\n",
    "        epoch_error += batch_error\n",
    "        num_batch = num_batch + 1\n",
    "\n",
    "    print('Epoch Error (epoch=%d): %.4f' % (epoch, epoch_error/num_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check whether the trained RBM model can extract features from the images.\n",
    "\n",
    "We calculate the probability of hidden nodes given input MNIST data as features and use a SciPy-based logistic regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('Extracting features...')\n",
    "\n",
    "train_features = np.zeros((len(train_dataset), HIDDEN_UNITS))\n",
    "train_labels = np.zeros(len(train_dataset))\n",
    "test_features = np.zeros((len(validation_dataset), HIDDEN_UNITS))\n",
    "test_labels = np.zeros(len(validation_dataset))\n",
    "\n",
    "for i, (batch, labels) in enumerate(train_loader):\n",
    "    batch = batch.view(len(batch), VISIBLE_UNITS)  # flatten input data\n",
    "    try:\n",
    "        train_features[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = rbm.compute_hidden(batch).numpy()\n",
    "        train_labels[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = labels.numpy()\n",
    "    except:\n",
    "        size = len(train_dataset) - i*BATCH_SIZE\n",
    "        train_features[i*BATCH_SIZE:len(train_dataset)] = rbm.compute_hidden(batch).numpy()[0:size]\n",
    "        train_labels[i*BATCH_SIZE:len(train_dataset)] = labels.numpy()[0:size]        \n",
    "\n",
    "for i, (batch, labels) in enumerate(validation_loader):\n",
    "    batch = batch.view(len(batch), VISIBLE_UNITS)  # flatten input data\n",
    "    try:\n",
    "        test_features[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = rbm.compute_hidden(batch).numpy()\n",
    "        test_labels[i*BATCH_SIZE:i*BATCH_SIZE+len(batch)] = labels.numpy()\n",
    "    except:\n",
    "        size = len(test_dataset) - i*BATCH_SIZE\n",
    "        test_features[i*BATCH_SIZE:len(test_dataset)] = rbm.compute_hidden(batch).numpy()[0:size]\n",
    "        test_labels[i*BATCH_SIZE:len(test_dataset)] = labels.numpy()[0:size]  \n",
    "\n",
    "\n",
    "########## CLASSIFICATION ##########\n",
    "print('Classifying...')\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_features, train_labels)\n",
    "predictions = clf.predict(test_features)\n",
    "\n",
    "print('Classification Accuracy: %d/%d' % (sum(predictions == test_labels), test_labels.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        ## initialize the paramaters with the weights and hidden_bias in trained RBM.\n",
    "        self.fc1.weight = nn.Parameter(torch.transpose(rbm.weights,1,0))\n",
    "        self.fc1.bias = nn.Parameter(rbm.hidden_bias)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        ## forward x to the first fully connected layer.\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        ## forward x to sigmoid activation function.\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        ## forward x to the second fully connected layer.\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        ## forward x to softmax activation function.\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "        \n",
    "        return x     \n",
    "    \n",
    "def train(epoch, log_interval=200):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        ## zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## pass data through the network\n",
    "        outputs = model(data)\n",
    "\n",
    "        ## calculate loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        ## backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        ## update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validate(loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        \n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))\n",
    "    \n",
    "## Trainig and evaluating 1) MLP\n",
    "model = Net1().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "## define the crosstntropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validate(lossv, accv)\n",
    "acc_result3 = []\n",
    "acc_result3.append(acc_result[0])\n",
    "acc_result3.append(accv[4])\n",
    "\n",
    "\n",
    "model_label2 = [\"without RBN\", \"with RBN\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Show barchart\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "bars = ax.bar(model_label2, acc_result3)\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x()+ 0.3, yval + 1, round(yval,2))\n",
    "ax.set_ylim(0, 100)\n",
    "plt.title('Accuracy of each network')\n",
    "plt.xlabel('Network')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
